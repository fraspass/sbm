# Estimating the stochastic blockmodel on a latent space of unknown dimension for an unknown number of communities

## Methodology

For simplicity, the embeddings will be generically denoted as <img alt="${\mathbf X}=[\boldsymbol x_1,\dots,\boldsymbol x_n]^\top\in\mathbb R^{n\times m},\ \boldsymbol x_i\in\mathbb R^m$" src="svgs/abe70a44538d121155dad760860b5070.svg" align="middle" width="266.52632985pt" height="27.9124395pt"/> for some <img alt="$m&gt;K$" src="svgs/ad64b477ecc18129fe9b235d89bd7a54.svg" align="middle" width="51.48773685pt" height="22.4657235pt"/>, and the non-identifiable latent position <img alt="$\boldsymbol v^\star_j$" src="svgs/3e0fa3ff3c5539cdc0e09aed76998444.svg" align="middle" width="16.6590534pt" height="22.638462pt"/> are more practically renamed <img alt="$\boldsymbol v_j$" src="svgs/56fa4aecdf044ac082a31373cc0ad81d.svg" align="middle" width="15.41952885pt" height="14.6118786pt"/>. The notation <img alt="$\boldsymbol x_{i:d}$" src="svgs/2aa5d0cfa9ac37116f11e0443c6d1e96.svg" align="middle" width="26.0487843pt" height="14.6118786pt"/> denotes the first <img alt="$d$" src="svgs/2103f85b8b1477f430fc407cad462224.svg" align="middle" width="8.55596445pt" height="22.8310566pt"/> elements <img alt="$(x_1,\dots,x_d)$" src="svgs/d9cf27ab8ea9978e067dc5b146a9a2a8.svg" align="middle" width="83.14428045pt" height="24.657534pt"/> of the vector <img alt="$\boldsymbol x_i$" src="svgs/70be273b745beb375dc544763c727b86.svg" align="middle" width="15.48422205pt" height="14.6118786pt"/>, and similarly <img alt="$\boldsymbol x_{id:}$" src="svgs/e1d3148ecf605c30c9fbd6ffa404e657.svg" align="middle" width="26.0487843pt" height="14.6118786pt"/> denotes the last <img alt="$m-d$" src="svgs/559de9b66c7e81ca756bd5280353c7e6.svg" align="middle" width="43.0802559pt" height="22.8310566pt"/> elements <img alt="$(x_{d+1},\dots,x_{m})$" src="svgs/1e99a6d2aca38f5684f5b4275d857823.svg" align="middle" width="104.9004957pt" height="24.657534pt"/> of the vector. We assume that, given the number of communities <img alt="$K$" src="svgs/d6328eaebbcd5c358f426dbea4bdbf70.svg" align="middle" width="15.13700595pt" height="22.4657235pt"/> and the latent dimension <img alt="$d$" src="svgs/2103f85b8b1477f430fc407cad462224.svg" align="middle" width="8.55596445pt" height="22.8310566pt"/>, the embeddings are generated from <img alt="$K$" src="svgs/d6328eaebbcd5c358f426dbea4bdbf70.svg" align="middle" width="15.13700595pt" height="22.4657235pt"/> <img alt="$d$" src="svgs/2103f85b8b1477f430fc407cad462224.svg" align="middle" width="8.55596445pt" height="22.8310566pt"/>-dimensional community-specific Gaussians in the first <img alt="$d$" src="svgs/2103f85b8b1477f430fc407cad462224.svg" align="middle" width="8.55596445pt" height="22.8310566pt"/> components, say <img alt="$\mathbf X_{:d}$" src="svgs/0fe2d5f1c4b80c5c9a473d32ea1c5285.svg" align="middle" width="24.8567286pt" height="22.5570873pt"/>, and from a generic <img alt="$(m-d)$" src="svgs/49f50be6c56b16056a48557243d76fa5.svg" align="middle" width="55.86568845pt" height="24.657534pt"/>-dimensional Gaussian (which is not cluster-specific) for the remaining components <img alt="$\mathbf X_{d:}$" src="svgs/999f5178905008d46d45806e3539fcfd.svg" align="middle" width="24.8567286pt" height="22.5570873pt"/>. Therefore, introducing latent community assignments <img alt="$\boldsymbol z=(z_1,\dots,z_n)$" src="svgs/0fbe66c1f30016944032b59dd3ffed07.svg" align="middle" width="112.6615215pt" height="24.657534pt"/>, the model can be expressed as follows:

<p align="center"><img alt="\begin{align*}&#10;\boldsymbol x_i \vert d,K,z_i,\boldsymbol v_{z_i},\bm\Sigma_{z_i}, \boldsymbol v_r, \bm\Sigma_r &amp;\overset{d}{\sim} \mathbb N_m \left( \begin{bmatrix} \boldsymbol v_{z_i} \\ \boldsymbol v_r \end{bmatrix}, \begin{bmatrix} \bm\Sigma_{z_i} &amp; \boldsymbol 0 \\ \boldsymbol 0 &amp; \bm\Sigma_r \end{bmatrix} \right),\ i=1,\dots,n, \\&#10;(\boldsymbol v_{k},\bm\Sigma_{k})\vert d, K &amp;\overset{iid}{\sim} \mathrm{NIW}_d(\boldsymbol v_{0:d},\kappa_0,\nu_0+d-1,\bm\Delta_{0:d}),\ k=1,\dots,K \\&#10;(\boldsymbol v_{r},\bm\Sigma_{r})\vert d, K &amp;\overset{d}{\sim} \mathrm{NIW}_{m-d}(\boldsymbol v_{0d:},\kappa_0,\nu_0+m-d-1,\bm\Delta_{0d:}), \\&#10;z_i\vert\bm\theta, K &amp;\overset{iid}{\sim}\mathrm{Multinoulli}(\bm\theta),\ i=1,\dots,n, \ \bm\theta\in\mathcal S_{K-1}, \\&#10;\bm\theta \vert K &amp;\overset{d}{\sim} \mathrm{Dirichlet}\left(\frac{\alpha}{K},\dots,\frac{\alpha}{K}\right), \\&#10;k &amp;\overset{d}{\sim} \mathrm{Geometric}(\omega), \\&#10;d &amp;\overset{d}{\sim} \mathrm{Geometric}(\delta).&#10;\end{align*}" src="svgs/829b38360a78f49a4debbad15ec506f6.svg" align="middle" width="542.66040015pt" height="231.6803577pt"/></p>

where <img alt="$\mathcal S_{K-1}$" src="svgs/1ca2006f4cd689950a74e72a91091a21.svg" align="middle" width="38.63261985pt" height="22.4657235pt"/> is the <img alt="$K-1$" src="svgs/877f17da0d7c756840fdc976d3c9e264.svg" align="middle" width="43.4474007pt" height="22.4657235pt"/> probability simplex. Note that the inverse Wishart has been partially re-parametrised using a parameter <img alt="$\nu_0&gt;0$" src="svgs/1bb6a4a3cd46c31f86edf629fac4d2f7.svg" align="middle" width="45.63157335pt" height="21.1872144pt"/> and adding the corresponding dimension to obtain the required constraint <img alt="$\nu&gt;d-1$" src="svgs/59d27f4580a6da5a1f33c7c387d77565.svg" align="middle" width="67.95067785pt" height="22.8310566pt"/> for the generic parametrisation and interpretation of <img alt="$\nu$" src="svgs/b49211c7e49541e500c32b4d56d354dc.svg" align="middle" width="9.16670205pt" height="14.1552444pt"/> in the inverse Wishart. Also note that <img alt="$m$" src="svgs/0e51a2dede42189d77627c4d742822c3.svg" align="middle" width="14.4331011pt" height="14.1552444pt"/> can be chosen to be equal to <img alt="$K$" src="svgs/d6328eaebbcd5c358f426dbea4bdbf70.svg" align="middle" width="15.13700595pt" height="22.4657235pt"/>, when fixed, for parsimony, or equal to <img alt="$n$" src="svgs/55a049b8f161ae7cfeb0197d75aff967.svg" align="middle" width="9.86687625pt" height="14.1552444pt"/> to have the maximum possible dimension of the embeddings.

## Understanding the code

The main part of the code is contained in the file `sampler_functions.py`, which includes the main proposals used in the MCMC sampler. The files `learn_sbm.py` and `learn_blockgauss.py` contain code to simulate graph embeddings or a block Gaussian.

<!--
## References

* Heard, N.A., Rubin-Delanchy, P.T.G. and Lawson, D.J. (2014). "Filtering automated polling traffic in computer network flow data". Proceedings - 2014 IEEE Joint Intelligence and Security Informatics Conference, JISIC 2014, 268-271. ([Link](https://ieeexplore.ieee.org/document/6975589/))
-->

